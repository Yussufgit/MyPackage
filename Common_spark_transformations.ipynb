{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yussufgit/MyPackage/blob/master/Common_spark_transformations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMRafIZdZIQx"
      },
      "source": [
        "# Common Transformations in Spark\n",
        "© Explore Data Science Academy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJqg1ofxZIQ3"
      },
      "source": [
        "## Learning objectives\n",
        "\n",
        "In this train, you will learn how to:\n",
        "\n",
        "- read data into Spark;\n",
        "- implement projections and filters;\n",
        "- rename, add and drop columns;\n",
        "- perform aggregations, and \n",
        "- save progress by writing data from Spark. \n",
        "\n",
        "## Outline\n",
        "\n",
        "To achieve our objectives, this train is structured as follows: \n",
        "\n",
        " - An introduction to the PySpark API;\n",
        " - Accessing our data using PySpark;\n",
        " - Altering column data;\n",
        " - Projections and filters;\n",
        " - Adding, renaming, and dropping columns; \n",
        " - Aggregations; and \n",
        " - Writing data from the environment using PySpark.    \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hai1N8sZIQ5"
      },
      "source": [
        "## An introduction to the PySpark API\n",
        "\n",
        "<div align=\"center\" style=\"width: 600px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
        "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Spark_and_python_s.png\"\n",
        "     alt=\"Dummy image 1\"\n",
        "     style=\"float: center; padding-bottom=0.5em\"\n",
        "     width=600px/>\n",
        "     Spark and Python, a powerful partnership we'll learn to master. Logos exist in the <a href=\"https://creativecommons.org/\"> Creative Commons</a> space.\n",
        "</div>\n",
        "\n",
        "To increase its access to a wider audience of users, Spark provides several language-specific APIs that can be used to interact with its underlying machinery. At the time of this train's writing, the officially supported APIs include implementations for [Python](https://spark.apache.org/docs/latest/api/python/index.html), [Scala](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/index.html), [SQL](https://spark.apache.org/docs/latest/api/sql/), [R](https://spark.apache.org/docs/latest/api/R/index.html), and [Java](https://spark.apache.org/docs/latest/api/java/index.html).  \n",
        "\n",
        "\n",
        "Due to the familiarity we've already gained with Python, in this train and from here onwards, we will focus on the Python implementation – Pyspark. \n",
        "\n",
        "As Spark (and therefore PySpark) has seen many version updates, with each varying the expected behaviour of the APIs, we will **prescribe the use of `PySpark 3.0.1+` (Spark 3.0.1) built on top of `Hadoop 2.7`** going forward. \n",
        "\n",
        "Remember that you can install PySpark through the installation of the complete Spark suite or by running:\n",
        "\n",
        "`$ pip install pyspark` \n",
        "\n",
        "PySpark has a similar API to other Python programming tools and, since Spark 2.0 and the adoption of `DataFrames`, has grown to be very familiar to anyone who has worked with the Pandas API. \n",
        "\n",
        "Before we go any further, let's invoke the `PySpark` API and check that its version is correct:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "fGQTVkmKZIQ8",
        "outputId": "04116624-55e0-4103-f962-d1fe4eb4c726"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 44 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 59.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=047eeb6b9d3a2d847da4dd2ab8b0356fce2a1ea84aa37a56e78e917cee458a22\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.3.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "import pyspark\n",
        "pyspark.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVabhZK5ZIQ_"
      },
      "source": [
        "While Spark can be run on a large cluster consisting of several interdependent nodes, we assume that by default this train will run with Spark in `local` mode, where the `driver` and `executors` will be located on the same machine. \n",
        "\n",
        "As we've seen, to allow Spark to execute our instructions through the PySpark API, we first need to register a `SparkContext` followed by setting up a `SparkSession`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "j-VE-PorZIRA"
      },
      "outputs": [],
      "source": [
        "# To start programming in Spark, define a SparkContext and SparkSession.\n",
        "\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "sc = SparkContext()\n",
        "spark = SparkSession(sc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mp4mfjGZIRB"
      },
      "source": [
        "With our session prepared, we're now ready to start reading in some data to analyse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0glQ6MQZIRC"
      },
      "source": [
        "## Accessing our data using PySpark\n",
        "\n",
        "In a field that is characterised by highly diverse sets of data that often need to be analysed, it is potentially risky to use a tool that is very particular about which data sources it supports. Fortunately for us, Spark has abstractions that allow for the consumption of data from several popular data sources, such as in-memory DataFrames, NoSQL databases, RDBMSs, and streaming applications.   \n",
        "\n",
        "To see an example of this in practice, let's load the dataset which we'll be using throughout this train. Here our data consist of recorded Bitcoin transactions that are sourced from Kaggle [here](https://www.kaggle.com/mczielinski/bitcoin-historical-data).\n",
        "\n",
        "\n",
        "<div align=\"center\" style=\"width: 600px; font-size: 80%; text-align: center; margin: 0 auto\">\n",
        "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/bitcoin.png\"\n",
        "     alt=\"Bitcoin-dataset\"\n",
        "     style=\"float: center; padding-bottom=0.5em\"\n",
        "     width=300px/>\n",
        "     \"Do you even Bitcoin?\" Image by <a href=\"https://pixabay.com/users/sinisamaric1-3044277/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3125488\">Sinisa Maric</a> from <a href=\"https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3125488\">Pixabay</a>\n",
        "</div>\n",
        "\n",
        "\n",
        "We aim to read this data in as a DataFrame object. Spark's DataFrameReader (`spark.read()` function) does this for us by reading in a file and returning a `DataFrame` object of rows and named columns. The data type held within each of these columns can either be defined through the use of a schema (more on this later on) or can be inferred at read time by Spark. \n",
        "\n",
        "Using the `SparkSession` object we defined in the previous cell, it's easy to call methods to read in data from CSV, or any other [supported data format](https://spark.apache.org/docs/latest/sql-data-sources.html). Here we are provided with several methods that can be strung together to alter the behaviour of our read task. These include: \n",
        "\n",
        " - the **`format()`** method, which can be used to designate the input file format;   \n",
        " - the **`option()`** method, which takes in key-value pairs to define various read behaviours, such as data integrity checking or header inclusion;    \n",
        " - the **`schema()`** method, which takes in either a `StructType` object or a [Data Definition Language](https://en.wikipedia.org/wiki/Data_definition_language) (`DDL`) string to define the data types expected in each DataFrame column; and   \n",
        " - the **`load()`** method, which receives a system path to the file(s) that are to be read. \n",
        " \n",
        "Let's put this all together by reading our data into a DataFrame which we'll call (generically) `df`. We'll include options to read in the data header and specify that our target source file is in a `csv` format: "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/archive.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjVuuSA2o8Vn",
        "outputId": "65646721-7d38-47c3-daaa-38cc318663c5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/archive.zip\n",
            "  inflating: bitstampUSD_1-min_data_2012-01-01_to_2021-03-31.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yFHgML8kZIRD"
      },
      "outputs": [],
      "source": [
        "# Remember to check that the path to the data given below is valid for your system.\n",
        "df = spark.read \\\n",
        "     .format('csv') \\\n",
        "     .option('header',True) \\\n",
        "     .load('/content/bitstampUSD_1-min_data_2012-01-01_to_2021-03-31.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6L2urn3ZIRF"
      },
      "source": [
        "To make our code more concise, PySpark also provides format-specific load operators, such as `spark.read.csv()`, which can be convenient if our read operation requires no special attributes and is in a consistent format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GMpJPF9pZIRI"
      },
      "outputs": [],
      "source": [
        "# Remember to check that the path to the data given below is valid for your system.\n",
        "df = spark.read.csv('/content/bitstampUSD_1-min_data_2012-01-01_to_2021-03-31.csv', header=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4bY22huZIRI"
      },
      "source": [
        "In Spark, you can use the `.show()` method on a DataFrame to have a look at its contents. The results, however, are not as intuitive as the `.head()` function in Pandas, and a lack of rich formatting makes them much more difficult to view."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMlbLbVWZIRK",
        "outputId": "b649f8b5-e3eb-48c0-8c2c-dd171175d526"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+----+----+-----+------------+-----------------+--------------+\n",
            "|Timestamp |Open|High|Low |Close|Volume_(BTC)|Volume_(Currency)|Weighted_Price|\n",
            "+----------+----+----+----+-----+------------+-----------------+--------------+\n",
            "|1325317920|4.39|4.39|4.39|4.39 |0.45558087  |2.0000000193     |4.39          |\n",
            "|1325317980|NaN |NaN |NaN |NaN  |NaN         |NaN              |NaN           |\n",
            "|1325318040|NaN |NaN |NaN |NaN  |NaN         |NaN              |NaN           |\n",
            "|1325318100|NaN |NaN |NaN |NaN  |NaN         |NaN              |NaN           |\n",
            "|1325318160|NaN |NaN |NaN |NaN  |NaN         |NaN              |NaN           |\n",
            "+----------+----+----+----+-----+------------+-----------------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# The `show()` method allows us to view the data, with the first argument specifying the number of rows to show, \n",
        "# and the second specifying that long entries should not be truncated. \n",
        "df.show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcKZaQCiZIRL"
      },
      "source": [
        "Spark has a `describe()` function which gives us a summarised view of the dataset. Importantly, this function doesn't automatically return a result, but instead produces a transformed version of our DataFrame which still  needs to be displayed:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fDnquUmZIRM",
        "outputId": "216f2720-4e9b-429a-bd5e-5a6dba61c248"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+-------+-------+-------+-------+------------+-----------------+--------------+\n",
            "|summary|Timestamp           |Open   |High   |Low    |Close  |Volume_(BTC)|Volume_(Currency)|Weighted_Price|\n",
            "+-------+--------------------+-------+-------+-------+-------+------------+-----------------+--------------+\n",
            "|count  |4857377             |4857377|4857377|4857377|4857377|4857377     |4857377          |4857377       |\n",
            "|mean   |1.4713007665042922E9|NaN    |NaN    |NaN    |NaN    |NaN         |NaN              |NaN           |\n",
            "|stddev |8.428019437554955E7 |NaN    |NaN    |NaN    |NaN    |NaN         |NaN              |NaN           |\n",
            "|min    |1325317920          |10     |10     |1.5    |1.5    |0           |0                |10            |\n",
            "|max    |1617148800          |NaN    |NaN    |NaN    |NaN    |NaN         |NaN              |NaN           |\n",
            "+-------+--------------------+-------+-------+-------+-------+------------+-----------------+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_desc = df.describe()\n",
        "df_desc.show(10, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2OrPUo2ZIRN"
      },
      "source": [
        "If we try to perform many operations on a single DataFrame, we'll soon find that the above syntax is cumbersome and adds needless bloat to our code. Thankfully, the PySpark API supports *chaining*, which allows us to string many operations together on a single line. \n",
        "\n",
        "See this in action, by performing the `describe()` operation again through chaining:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FR8bdkyPZIRN",
        "outputId": "8406119b-41e5-47a3-e5a9-bc67a2b85ee4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+-------+-------+-------+-------+------------+-----------------+--------------+\n",
            "|summary|Timestamp           |Open   |High   |Low    |Close  |Volume_(BTC)|Volume_(Currency)|Weighted_Price|\n",
            "+-------+--------------------+-------+-------+-------+-------+------------+-----------------+--------------+\n",
            "|count  |4857377             |4857377|4857377|4857377|4857377|4857377     |4857377          |4857377       |\n",
            "|mean   |1.4713007665042922E9|NaN    |NaN    |NaN    |NaN    |NaN         |NaN              |NaN           |\n",
            "|stddev |8.428019437554955E7 |NaN    |NaN    |NaN    |NaN    |NaN         |NaN              |NaN           |\n",
            "|min    |1325317920          |10     |10     |1.5    |1.5    |0           |0                |10            |\n",
            "|max    |1617148800          |NaN    |NaN    |NaN    |NaN    |NaN         |NaN              |NaN           |\n",
            "+-------+--------------------+-------+-------+-------+-------+------------+-----------------+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We can call the .show() method directly on the transformed DataFrame produced from .describe().\n",
        "df.describe().show(10, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNSm-GYJZIRO"
      },
      "source": [
        "Similar to Pandas, Spark also allows us to have a look at the data types using the `.dtypes` attribute of a DataFrame, or by calling its `printSchema()` method: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8y5BTPVLZIRO",
        "outputId": "29b0c0a0-beb2-4dc6-ee92-bafb55e46932"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Timestamp', 'string'),\n",
              " ('Open', 'string'),\n",
              " ('High', 'string'),\n",
              " ('Low', 'string'),\n",
              " ('Close', 'string'),\n",
              " ('Volume_(BTC)', 'string'),\n",
              " ('Volume_(Currency)', 'string'),\n",
              " ('Weighted_Price', 'string')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7Ah29YVZIRP",
        "outputId": "8114c2b2-fe93-48d3-8d84-9f2db774f81f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Timestamp: string (nullable = true)\n",
            " |-- Open: string (nullable = true)\n",
            " |-- High: string (nullable = true)\n",
            " |-- Low: string (nullable = true)\n",
            " |-- Close: string (nullable = true)\n",
            " |-- Volume_(BTC): string (nullable = true)\n",
            " |-- Volume_(Currency): string (nullable = true)\n",
            " |-- Weighted_Price: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "050zLS_nZIRQ"
      },
      "source": [
        "From the above output, we can see that all the columns within our DataFrame were inferred incorrectly as `string` data types. This is an example of Spark's best (although unfortunately inadequate) efforts to automatically infer data types.  \n",
        "\n",
        "To better read in our data, we can manually specify the schema of the dataset through built-in data types used in Spark. Remember that `StructType()` defines the overall structure of a DataFrame, and `StructField()` defines the content of a column – containing a field name, data type, and if the field is nullable. \n",
        "\n",
        "So let's go ahead and define a schema for our dataset. To do so, we make use of the various data types provided within the `pyspark.sql.types` module (a full list of the supported types and their properties can be found [here](https://spark.apache.org/docs/latest/sql-ref-datatypes.html)).  \n",
        "\n",
        "*Note that below we read timestamps in as Integer types, and then later cast them to Timestamp types. We do this as the Spark reader does not perfectly cast date types at read time.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XbGDiMWWZIRR"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StringType, StructType, StructField, IntegerType, FloatType\n",
        "\n",
        "schema = StructType([StructField('Timestamp', IntegerType(), True),\n",
        "                     StructField('Open', FloatType(), True),\n",
        "                     StructField('High', FloatType(), True),\n",
        "                     StructField('Low', FloatType(), True),\n",
        "                     StructField('Close', FloatType(), True),\n",
        "                     StructField('Volume_(BTC)', FloatType(), True),\n",
        "                     StructField('Volume_(Currency)', FloatType(), True),\n",
        "                     StructField('Weighted_Price', FloatType(), True)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S95e8FTyZIRR"
      },
      "source": [
        "With our freshly defined schema, let's read in the data again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FGeHea5zZIRS"
      },
      "outputs": [],
      "source": [
        "df = spark.read.csv('/content/bitstampUSD_1-min_data_2012-01-01_to_2021-03-31.csv', header=True, schema=schema, multiLine=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akxozy4NZIRT"
      },
      "source": [
        "We now check the DataFrame schema for correctness: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZdq3XFVZIRT",
        "outputId": "319f4f42-a6a1-4026-9fc9-1e27be72ea46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Timestamp: integer (nullable = true)\n",
            " |-- Open: float (nullable = true)\n",
            " |-- High: float (nullable = true)\n",
            " |-- Low: float (nullable = true)\n",
            " |-- Close: float (nullable = true)\n",
            " |-- Volume_(BTC): float (nullable = true)\n",
            " |-- Volume_(Currency): float (nullable = true)\n",
            " |-- Weighted_Price: float (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HU5_QDZZIRT"
      },
      "source": [
        "Success! \n",
        "\n",
        "With a correct schema, let's take a look at our data again, this time paying special attention to the `Timestamp` field we mentioned earlier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iTyKrFLZIRU",
        "outputId": "20787274-bbb8-4cd4-e1d3-57b119f45561"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+-------+-------+-------+-------+------------+-----------------+--------------+\n",
            "|summary|Timestamp           |Open   |High   |Low    |Close  |Volume_(BTC)|Volume_(Currency)|Weighted_Price|\n",
            "+-------+--------------------+-------+-------+-------+-------+------------+-----------------+--------------+\n",
            "|count  |4857377             |4857377|4857377|4857377|4857377|4857377     |4857377          |4857377       |\n",
            "|mean   |1.4713007665042922E9|NaN    |NaN    |NaN    |NaN    |NaN         |NaN              |NaN           |\n",
            "|stddev |8.428019437554696E7 |NaN    |NaN    |NaN    |NaN    |NaN         |NaN              |NaN           |\n",
            "|min    |1325317920          |3.8    |3.8    |1.5    |1.5    |0.0         |0.0              |3.8           |\n",
            "|max    |1617148800          |NaN    |NaN    |NaN    |NaN    |NaN         |NaN              |NaN           |\n",
            "+-------+--------------------+-------+-------+-------+-------+------------+-----------------+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.describe().show(10, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TftbVfH9ZIRU",
        "outputId": "ffa9f3f5-7606-4012-aeb2-cc05e27b1f9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+----+----+-----+------------+-----------------+--------------+\n",
            "| Timestamp|Open|High| Low|Close|Volume_(BTC)|Volume_(Currency)|Weighted_Price|\n",
            "+----------+----+----+----+-----+------------+-----------------+--------------+\n",
            "|1325317920|4.39|4.39|4.39| 4.39|  0.45558086|              2.0|          4.39|\n",
            "|1325317980| NaN| NaN| NaN|  NaN|         NaN|              NaN|           NaN|\n",
            "|1325318040| NaN| NaN| NaN|  NaN|         NaN|              NaN|           NaN|\n",
            "|1325318100| NaN| NaN| NaN|  NaN|         NaN|              NaN|           NaN|\n",
            "|1325318160| NaN| NaN| NaN|  NaN|         NaN|              NaN|           NaN|\n",
            "+----------+----+----+----+-----+------------+-----------------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elX1iCIsZIRV"
      },
      "source": [
        "From the above output, everything appears to be correct except for the `Timestamp` column. Our goal, therefore, is to cast this field to the appropriate type. However, to do this we first have to better understand how we *manipulate data* in PySpark."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzDwhqMaZIRV"
      },
      "source": [
        "## Altering column data\n",
        "\n",
        "Manipulating the contents of a column, such as redefining its data type, is a common operation in Spark. To perform such a step, we call the `withColumn()` method on a DataFrame object. This method takes in two arguments:\n",
        "\n",
        " - The first is the **`colName`**, or \"column name\" argument, which is a string indicating the column we would like to manipulate. If we provide the name of an existing column within our DataFrame, the method will essentially update its contents. Alternatively, if we provide a new column name, the method will append this new column to the resulting DataFrame. \n",
        " - The second argument is the **`col`**, or \"column expression\", which defines what operation(s) should be performed to update/create the resulting column.  \n",
        "\n",
        "In our instance, we want to update the existing Timestamp column; changing the data from an `integer` encoding to a more suitable `timestamp` format. \n",
        "\n",
        "To perform this conversion, we can use a powerful set of functions provided within PySpark's [`functions` module](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions). Here we are specifically interested in the `to_timestamp()` and `to_date()` functions, however, we'll soon see that we make extensive use of this module for many tasks.       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "oWWFm6JwZIRW"
      },
      "outputs": [],
      "source": [
        "# We use the conventional alias `F` for the functions module due to its heavy usage within our code.\n",
        "from pyspark.sql import functions as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWi57RasZIRW"
      },
      "source": [
        "Let's put our understanding together and update our `Timestamp` column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "mIzALojjZIRW"
      },
      "outputs": [],
      "source": [
        "df = df.withColumn(colName='Timestamp', col=F.to_timestamp(df['Timestamp']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmvD4K-UZIRX"
      },
      "source": [
        "Take a second to note how we selected the `Timestamp` column. Here we referenced the field using the `df['Timestamp']` syntax, which should feel familiar to us due to its similarity to Pandas. \n",
        "\n",
        "Interestingly, Spark actually provides several ways in which we can reference columns. \n",
        "\n",
        "These include using the `functions` module:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weurWnlpZIRX",
        "outputId": "8c7dfce3-174f-4958-8e3d-dd25cadb8ee2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Column<'Timestamp'>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "F.col('Timestamp')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4agwFyjfZIRY"
      },
      "source": [
        "Using Pandas notation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F66I98pBZIRY",
        "outputId": "5d09c303-33cf-4355-c88f-8136c640b56c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Column<'Timestamp'>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "df['Timestamp']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CFEDA4BZIRZ"
      },
      "source": [
        "Or applying dot notation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M37IMJ5EZIRZ",
        "outputId": "688524cc-239a-4d42-ae5b-d31908212c40"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Column<'Timestamp'>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "df.Timestamp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C2uaw2WZIRZ"
      },
      "source": [
        "Each of these is equivalent and will return the same `Column` object to us. \n",
        "\n",
        "Moving along, let's inspect the results of the transformation we just performed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2ySgb3wZIRa",
        "outputId": "a0e081c8-80ca-4d2b-99d8-2f89e03c25e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+----+----+----+-----+------------+-----------------+--------------+\n",
            "|Timestamp          |Open|High|Low |Close|Volume_(BTC)|Volume_(Currency)|Weighted_Price|\n",
            "+-------------------+----+----+----+-----+------------+-----------------+--------------+\n",
            "|2011-12-31 07:52:00|4.39|4.39|4.39|4.39 |0.45558086  |2.0              |4.39          |\n",
            "|2011-12-31 07:53:00|NaN |NaN |NaN |NaN  |NaN         |NaN              |NaN           |\n",
            "|2011-12-31 07:54:00|NaN |NaN |NaN |NaN  |NaN         |NaN              |NaN           |\n",
            "|2011-12-31 07:55:00|NaN |NaN |NaN |NaN  |NaN         |NaN              |NaN           |\n",
            "|2011-12-31 07:56:00|NaN |NaN |NaN |NaN  |NaN         |NaN              |NaN           |\n",
            "+-------------------+----+----+----+-----+------------+-----------------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show(5, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEFYjATbZIRa"
      },
      "source": [
        "As can be seen, the `Timestamp` column is now appropriately formatted, with its underlying data type being `TimestampType`. Using this data type, Spark allows us to access parts of the timestamp using other functions in the pyspark.sql.functions module,  such as variants on `day()`, `month()`, and `year()`. \n",
        "\n",
        "Using the `withColumn()` method, let's extract the year into a new column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "-NINjfASZIRa"
      },
      "outputs": [],
      "source": [
        "# Remember, F.col('Timestamp') is equivalent to df['Timestamp'].\n",
        "df = df.withColumn('Year', F.year(F.col('Timestamp')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRcSBVLKZIRb"
      },
      "source": [
        "And the results are: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPuc_UJyZIRb",
        "outputId": "96a1d619-62ff-41a5-e2e8-a444f3c1fc7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+----+----+----+-----+------------+-----------------+--------------+----+\n",
            "|Timestamp          |Open|High|Low |Close|Volume_(BTC)|Volume_(Currency)|Weighted_Price|Year|\n",
            "+-------------------+----+----+----+-----+------------+-----------------+--------------+----+\n",
            "|2011-12-31 07:52:00|4.39|4.39|4.39|4.39 |0.45558086  |2.0              |4.39          |2011|\n",
            "|2011-12-31 07:53:00|NaN |NaN |NaN |NaN  |NaN         |NaN              |NaN           |2011|\n",
            "|2011-12-31 07:54:00|NaN |NaN |NaN |NaN  |NaN         |NaN              |NaN           |2011|\n",
            "|2011-12-31 07:55:00|NaN |NaN |NaN |NaN  |NaN         |NaN              |NaN           |2011|\n",
            "|2011-12-31 07:56:00|NaN |NaN |NaN |NaN  |NaN         |NaN              |NaN           |2011|\n",
            "+-------------------+----+----+----+-----+------------+-----------------+--------------+----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show(5, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZpbOzZKZIRb"
      },
      "source": [
        "**\\[Exercise 1\\]**\n",
        "\n",
        "To familiarise yourself with the above functionality and syntax that we've covered so far, see if you can add two new columns to the above DataFrame called `Month`, and `Weekday`, each containing the appropriate data derived from the `Timestamp` column:   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "4U3X3xPTZIRc"
      },
      "outputs": [],
      "source": [
        "# Write your code to add a 'Month' column to the DataFrame here:\n",
        "df_ex1 = df.withColumn('Month', F.month(F.col('Timestamp')))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qob3_fthZIRc",
        "outputId": "2d783c81-9359-4e18-94d5-b278e3d83194"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+----+----+----+-----+------------+-----------------+--------------+----+-----+\n",
            "|Timestamp          |Open|High|Low |Close|Volume_(BTC)|Volume_(Currency)|Weighted_Price|Year|Month|\n",
            "+-------------------+----+----+----+-----+------------+-----------------+--------------+----+-----+\n",
            "|2011-12-31 07:52:00|4.39|4.39|4.39|4.39 |0.45558086  |2.0              |4.39          |2011|12   |\n",
            "|2011-12-31 07:53:00|NaN |NaN |NaN |NaN  |NaN         |NaN              |NaN           |2011|12   |\n",
            "|2011-12-31 07:54:00|NaN |NaN |NaN |NaN  |NaN         |NaN              |NaN           |2011|12   |\n",
            "|2011-12-31 07:55:00|NaN |NaN |NaN |NaN  |NaN         |NaN              |NaN           |2011|12   |\n",
            "|2011-12-31 07:56:00|NaN |NaN |NaN |NaN  |NaN         |NaN              |NaN           |2011|12   |\n",
            "+-------------------+----+----+----+-----+------------+-----------------+--------------+----+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_ex1.show(5, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "B4FXzYmMZIRc"
      },
      "outputs": [],
      "source": [
        "# Write your code to add a 'Weekday' column to the DataFrame here: \n",
        "# Note that the weekday can be represented as an integer, i.e. Sunday --> 1, Saturday --> 7.\n",
        "df_ex1 = df.withColumn('Weekday', F.dayofweek(F.col('Timestamp')))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tlf0T0WvZIRd",
        "outputId": "7d5ebd1a-d91c-4472-cbd2-e0a38e08ce20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+----+----+----+-----+------------+-----------------+--------------+----+-----+-------+\n",
            "|Timestamp          |Open|High|Low |Close|Volume_(BTC)|Volume_(Currency)|Weighted_Price|Year|Month|Weekday|\n",
            "+-------------------+----+----+----+-----+------------+-----------------+--------------+----+-----+-------+\n",
            "|2011-12-31 07:52:00|4.39|4.39|4.39|4.39 |0.45558086  |2.0              |4.39          |2011|12   |7      |\n",
            "|2011-12-31 07:53:00|NaN |NaN |NaN |NaN  |NaN         |NaN              |NaN           |2011|12   |7      |\n",
            "|2011-12-31 07:54:00|NaN |NaN |NaN |NaN  |NaN         |NaN              |NaN           |2011|12   |7      |\n",
            "|2011-12-31 07:55:00|NaN |NaN |NaN |NaN  |NaN         |NaN              |NaN           |2011|12   |7      |\n",
            "|2011-12-31 07:56:00|NaN |NaN |NaN |NaN  |NaN         |NaN              |NaN           |2011|12   |7      |\n",
            "+-------------------+----+----+----+-----+------------+-----------------+--------------+----+-----+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_ex1.show(5, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xy6qhaGgZIRd"
      },
      "source": [
        "## Projections and filters\n",
        "\n",
        "We now turn our attention to an important operation that we'll be using extensively in PySpark, namely projection. \n",
        "\n",
        "A *projection* in a relational sense is a way to return only the rows matching specific conditions. In PySpark, projections are done using the `select` method, where filters (that are used to specify the criteria for selection) are expressed using the `filter()` or `where()` methods, which are both equivalent. \n",
        "\n",
        "To get a feel for how this works, let's try to implement a projection by selecting all entries which occur within our data after 2019.\n",
        "\n",
        "To do this, we're going to use a couple of functions from the `datetime` module that you hopefully should be familiar with. Here we'll construct a `date` object, which can be used to filter `timestamp` data in Spark. \n",
        "Note that Spark also allows filtering with strings, for example, using `> '2020-01'` to return all dates greater than Jan-01-2020, but it is generally suggested to give the correct type to avoid possible data type mismatch errors between PySpark and Spark. \n",
        "\n",
        "Time to get filtering:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "HD4qZy3dZIRe"
      },
      "outputs": [],
      "source": [
        "# Create our date object for filtering.\n",
        "from datetime import date\n",
        "filter_date = date(2020, 1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OETq7ljJZIRe",
        "outputId": "973f9960-4d73-4808-86f1-259da31a80d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-------+-------+-------+-------+------------+-----------------+--------------+----+-----+\n",
            "|          Timestamp|   Open|   High|    Low|  Close|Volume_(BTC)|Volume_(Currency)|Weighted_Price|Year|Month|\n",
            "+-------------------+-------+-------+-------+-------+------------+-----------------+--------------+----+-----+\n",
            "|2020-01-01 00:01:00|7161.51|7161.51|7155.09| 7161.2|   3.7769244|        27047.305|      7161.198|2020|    1|\n",
            "|2020-01-01 00:02:00|7158.82|7158.82|7158.82|7158.82|  0.02927792|        209.59535|       7158.82|2020|    1|\n",
            "|2020-01-01 00:03:00|7158.82|7158.82| 7156.9| 7156.9|  0.06581935|         471.1561|      7158.322|2020|    1|\n",
            "|2020-01-01 00:04:00| 7158.5| 7158.5|7154.97| 7157.2|  0.97138673|         6950.501|      7155.236|2020|    1|\n",
            "|2020-01-01 00:05:00|7156.52|7159.51| 7150.1| 7158.5|  0.88693166|         6342.851|      7151.454|2020|    1|\n",
            "+-------------------+-------+-------+-------+-------+------------+-----------------+--------------+----+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.where(F.col('Timestamp') > filter_date).show(5, True)  # <-- Return all entries later than Jan 01, 2020."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "treJ2q4zZIRe"
      },
      "source": [
        "Nice! It looks like everything worked as expected. \n",
        "\n",
        "We've just filtered based on the row values found within a column, with the resulting DataFrame having its full complement of fields. Often, however, we'd like to constrain the columns we return as part of the projection as well. \n",
        "\n",
        "To do this we make use of the `select()` method, which allows us to pick a subset of the fields from the target DataFrame:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WY9N8rMnZIRf",
        "outputId": "f8491c94-8068-4f27-983b-5a8cf4e6b118"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+----+----+----+\n",
            "|Timestamp          |Open|High|Low |\n",
            "+-------------------+----+----+----+\n",
            "|2011-12-31 07:52:00|4.39|4.39|4.39|\n",
            "|2011-12-31 07:53:00|NaN |NaN |NaN |\n",
            "|2011-12-31 07:54:00|NaN |NaN |NaN |\n",
            "|2011-12-31 07:55:00|NaN |NaN |NaN |\n",
            "|2011-12-31 07:56:00|NaN |NaN |NaN |\n",
            "+-------------------+----+----+----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select('Timestamp', 'Open', 'High', 'Low').show(5, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSdLMi1MZIRf"
      },
      "source": [
        "As we've previously seen, we can chain this method together with our previously built filter to produce a concise projection: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb1Qex1ZZIRg",
        "outputId": "6c14fa12-72e7-41b1-806c-814c30d76704"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-------+-------+-------+\n",
            "|Timestamp          |Open   |High   |Low    |\n",
            "+-------------------+-------+-------+-------+\n",
            "|2020-01-01 00:01:00|7161.51|7161.51|7155.09|\n",
            "|2020-01-01 00:02:00|7158.82|7158.82|7158.82|\n",
            "|2020-01-01 00:03:00|7158.82|7158.82|7156.9 |\n",
            "|2020-01-01 00:04:00|7158.5 |7158.5 |7154.97|\n",
            "|2020-01-01 00:05:00|7156.52|7159.51|7150.1 |\n",
            "+-------------------+-------+-------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select('Timestamp', 'Open', 'High', 'Low').where(F.col('Timestamp') > filter_date).show(5, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgWxVQG0ZIRh"
      },
      "source": [
        "Nothing is stopping us from defining multiple filters within our projection. To do this, we simply need to use binary logic operators to define how our filters are to be combined.\n",
        "\n",
        "To demonstrate how this can be done, let's create a projection which only contains entries recorded after `2020-01-01`, *and* whose `'Open'` field value is greater than 7000: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BRhmX_7ZIRi",
        "outputId": "59a017f9-b54b-4f3a-b432-068be134e29d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-------+-------+-------+-------+------------+-----------------+--------------+----+-----+\n",
            "|Timestamp          |Open   |High   |Low    |Close  |Volume_(BTC)|Volume_(Currency)|Weighted_Price|Year|Month|\n",
            "+-------------------+-------+-------+-------+-------+------------+-----------------+--------------+----+-----+\n",
            "|2020-01-01 00:01:00|7161.51|7161.51|7155.09|7161.2 |3.7769244   |27047.305        |7161.198      |2020|1    |\n",
            "|2020-01-01 00:02:00|7158.82|7158.82|7158.82|7158.82|0.02927792  |209.59535        |7158.82       |2020|1    |\n",
            "|2020-01-01 00:03:00|7158.82|7158.82|7156.9 |7156.9 |0.06581935  |471.1561         |7158.322      |2020|1    |\n",
            "|2020-01-01 00:04:00|7158.5 |7158.5 |7154.97|7157.2 |0.97138673  |6950.501         |7155.236      |2020|1    |\n",
            "|2020-01-01 00:05:00|7156.52|7159.51|7150.1 |7158.5 |0.88693166  |6342.851         |7151.454      |2020|1    |\n",
            "+-------------------+-------+-------+-------+-------+------------+-----------------+--------------+----+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "filter_1 = F.col('Timestamp') > filter_date\n",
        "filter_2 = F.col('Open') > 7000.0\n",
        "# We use the '&' symbol to define the logical intersection of our filters. \n",
        "df.where(filter_1 & filter_2).show(5, False) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUohkgWPZIRi"
      },
      "source": [
        "It's important to note from the above syntax that we use *binary operators* (`&` for 'and', `|` for 'or', `~` for 'not') to combine filtering conditions, instead of Python's built-in logical operators (`and`, `or`, `not`). Using the latter operators will raise an error, as the underlying data structures being compared are not boolean values (which is the data type that Python's logical operators expect). \n",
        "\n",
        "Similar to previous examples, we can also write the above projection compactly by using chaining, while also selecting a subset of the DataFrame's columns: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2DMwESKZIRj",
        "outputId": "679dfeae-b1a5-4203-fb63-74ac1293986f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-------+-------+-------+\n",
            "|Timestamp          |Open   |High   |Close  |\n",
            "+-------------------+-------+-------+-------+\n",
            "|2020-01-01 00:01:00|7161.51|7161.51|7161.2 |\n",
            "|2020-01-01 00:02:00|7158.82|7158.82|7158.82|\n",
            "|2020-01-01 00:03:00|7158.82|7158.82|7156.9 |\n",
            "|2020-01-01 00:04:00|7158.5 |7158.5 |7157.2 |\n",
            "|2020-01-01 00:05:00|7156.52|7159.51|7158.5 |\n",
            "+-------------------+-------+-------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select('Timestamp', 'Open', 'High', 'Close')\\\n",
        "  .where((F.col('Timestamp') > filter_date) & (\n",
        "      F.col('Open') > 7000))\\\n",
        "  .show(5, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6arW8tZZIRj"
      },
      "source": [
        "Note the presence of double brackets in the above statement, which are used to logically separate our filtering conditions that we've defined inline. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAsog38lZIRk"
      },
      "source": [
        "**\\[Exercise 2\\]**\n",
        "\n",
        "To test your understanding of the content we've covered so far, we challenge you to try and create a projection containing all applicable entries within the data which: \n",
        " - occur on a Monday after Dec 31, 2017; and\n",
        " - have a 'Close' value less than 6000. \n",
        "\n",
        "To neaten the results of your projection, display only the 'Timestamp' and 'Close' columns in your answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "gmRtEFB5ZIRl"
      },
      "outputs": [],
      "source": [
        "# Write your code to define your data projection here:\n",
        "new_filter_date= date(2017,12,31)\n",
        "filter_1 = F.dayofweek(F.col('Timestamp')) == 2\n",
        "filter_3 = F.col('Timestamp') > new_filter_date\n",
        "filter_2 = F.col('Close') < 6000\n",
        "df_ex2 = df.select('Timestamp', 'Close').where((filter_1) & (filter_2) & (filter_3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0smV6naZIRl",
        "outputId": "a67d60d4-b720-4163-fa6a-a617d1478b9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-------+\n",
            "|Timestamp          |Close  |\n",
            "+-------------------+-------+\n",
            "|2018-11-19 00:00:00|5559.15|\n",
            "|2018-11-19 00:01:00|5547.13|\n",
            "|2018-11-19 00:02:00|5551.14|\n",
            "|2018-11-19 00:03:00|5549.3 |\n",
            "|2018-11-19 00:04:00|5549.29|\n",
            "+-------------------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_ex2.show(5, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exCwTujrZIRm"
      },
      "source": [
        "## Adding, renaming, and dropping columns\n",
        "\n",
        "To round off our understanding of DataFrame manipulation, let's consider some Spark functionality to alter the *structure* of a DataFrame.  \n",
        "\n",
        "Spark allows us to rename, add, or drop columns in DataFrames. While we've already looked at how to add columns to a DataFrame through the use of the `withColumn()` method,  in the following subsections, we'll consider both the motivation and methods behind column renaming and removal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOySDrtkZIRm"
      },
      "source": [
        "### Renaming a column \n",
        "\n",
        "There may be limitless reasons why we may want to rename some or all of the columns within a given DataFrame. For a data engineer, however, the motivation behind renaming one or more columns typically arises from two frequent scenarios: \n",
        "\n",
        " - **Maintaining a naming convention:** It is often the case that our data integrates into a larger system or is stored in a specific file format that requires a certain naming convention to be upheld. For example, the Parquet file format (which will be discussed in an upcoming section) does not allow for space characters within column names. \n",
        " \n",
        " - **To increase readability:** While we may become very familiar with the datasets we handle on a day-to-day basis, our ultimate role as data engineers is to help *other* teams, individuals, and systems to make effective use of available data. To fulfil this mandate, we may want to improve the readability of our DataFrame by including more descriptive column headings, normalising capitalisation, or removing non-alphanumeric characters which may be problematic for data scientists and analysts to work with.   \n",
        "\n",
        "\n",
        "To put these principles into practice, let's consider our current DataFrame and think of ways in which we can improve its column headings: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOz9eFjKZIRn",
        "outputId": "ece84f45-3b99-41f9-dbec-ce6df406a34e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Timestamp',\n",
              " 'Open',\n",
              " 'High',\n",
              " 'Low',\n",
              " 'Close',\n",
              " 'Volume_(BTC)',\n",
              " 'Volume_(Currency)',\n",
              " 'Weighted_Price',\n",
              " 'Year',\n",
              " 'Month']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "# We can use the .columns attribute of a DataFrame to conveniently see its field headings.\n",
        "df.columns "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si2SnFlmZIRn"
      },
      "source": [
        "Looking at the current data, two potential issues need our attention: \n",
        "\n",
        " - Each column is capitalised, which may defy our organisation's naming convention.\n",
        " - The `Volume_(BTC)` and `Volume_(Currency)` fields contain non-alphanumeric characters (brackets) which may impede the efforts of data scientists working with the data. \n",
        "\n",
        "To rename columns in a DataFrame, we use the `withColumnRenamed()` method. This method works similarly to the `withColumn()` method and takes in two string arguments – the column to rename and the new name it should be designated by.\n",
        "\n",
        "Let's see this in action: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "ba0NMgIKZIRn"
      },
      "outputs": [],
      "source": [
        "new_df = df.withColumnRenamed('Timestamp', 'timestamp')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoy1K2mVZIRo"
      },
      "source": [
        "Remember that Python DataFrames and RDDs are immutable. Thus, even if we rename a column, or we use `withColumn` to manipulate data within a DataFrame, we are not actually renaming the column or performing the transformation on the original DataFrame, but are rather creating a new object containing the changes. So if we want to persist the changes, we have to instantiate a new object which will receive these (shown as `new_df` in the cell above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9wrSqZEZIRo",
        "outputId": "e8b7259b-8909-4444-ec34-35045ad71b78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+----+----+----+-----+------------+-----------------+--------------+----+-----+\n",
            "|          timestamp|Open|High| Low|Close|Volume_(BTC)|Volume_(Currency)|Weighted_Price|Year|Month|\n",
            "+-------------------+----+----+----+-----+------------+-----------------+--------------+----+-----+\n",
            "|2011-12-31 07:52:00|4.39|4.39|4.39| 4.39|  0.45558086|              2.0|          4.39|2011|   12|\n",
            "|2011-12-31 07:53:00| NaN| NaN| NaN|  NaN|         NaN|              NaN|           NaN|2011|   12|\n",
            "|2011-12-31 07:54:00| NaN| NaN| NaN|  NaN|         NaN|              NaN|           NaN|2011|   12|\n",
            "|2011-12-31 07:55:00| NaN| NaN| NaN|  NaN|         NaN|              NaN|           NaN|2011|   12|\n",
            "|2011-12-31 07:56:00| NaN| NaN| NaN|  NaN|         NaN|              NaN|           NaN|2011|   12|\n",
            "+-------------------+----+----+----+-----+------------+-----------------+--------------+----+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "new_df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zlaqv5IaZIRo"
      },
      "source": [
        "Here we can see the newly updated column name, along with the other columns which are still capitalised. \n",
        "\n",
        "If we want to apply the same transformation to all the columns within our DataFrame, we might want to complete the process programmatically in Python to make the transformation generic (and save some time).\n",
        "\n",
        "Let's see how we can do this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDw0_bW7ZIRp",
        "outputId": "3f235e32-ff99-476d-b461-0fe9c2f4e59c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+----+----+----+-----+------------+-----------------+--------------+----+-----+\n",
            "|          timestamp|open|high| low|close|volume_(btc)|volume_(currency)|weighted_price|year|month|\n",
            "+-------------------+----+----+----+-----+------------+-----------------+--------------+----+-----+\n",
            "|2011-12-31 07:52:00|4.39|4.39|4.39| 4.39|  0.45558086|              2.0|          4.39|2011|   12|\n",
            "|2011-12-31 07:53:00| NaN| NaN| NaN|  NaN|         NaN|              NaN|           NaN|2011|   12|\n",
            "|2011-12-31 07:54:00| NaN| NaN| NaN|  NaN|         NaN|              NaN|           NaN|2011|   12|\n",
            "|2011-12-31 07:55:00| NaN| NaN| NaN|  NaN|         NaN|              NaN|           NaN|2011|   12|\n",
            "|2011-12-31 07:56:00| NaN| NaN| NaN|  NaN|         NaN|              NaN|           NaN|2011|   12|\n",
            "+-------------------+----+----+----+-----+------------+-----------------+--------------+----+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Loop over each column name within our DataFrame.\n",
        "for column in df.columns:\n",
        "    df = df.withColumnRenamed(column, '_'.join(column.split()).lower())\n",
        "# Display our results.\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzi9u6W4ZIRp"
      },
      "source": [
        "There we go. Each column is now in lower case. \n",
        "\n",
        "We aren't done though, as we still have to address the presence of non-alphanumeric characters within the headings. \n",
        "\n",
        "**\\[Exercise 3\\]**\n",
        "\n",
        "Before we give the code to perform the removal of the non-alphanumeric characters, think about how you would alter the above code to get rid of the `(` and `)` characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTW42rkIZIRp",
        "outputId": "975f064c-1f11-40dc-d6b2-564c6cd384d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+----+----+----+-----+----------+---------------+--------------+----+-----+\n",
            "|          timestamp|open|high| low|close|volume_btc|volume_currency|weighted_price|year|month|\n",
            "+-------------------+----+----+----+-----+----------+---------------+--------------+----+-----+\n",
            "|2011-12-31 07:52:00|4.39|4.39|4.39| 4.39|0.45558086|            2.0|          4.39|2011|   12|\n",
            "|2011-12-31 07:53:00| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011|   12|\n",
            "|2011-12-31 07:54:00| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011|   12|\n",
            "|2011-12-31 07:55:00| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011|   12|\n",
            "|2011-12-31 07:56:00| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011|   12|\n",
            "+-------------------+----+----+----+-----+----------+---------------+--------------+----+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Write your code here to remove the '(' and ')' characters from the column headings:\n",
        "for column in df.columns:\n",
        "    df = df.withColumnRenamed(column, '_'.join(column.split()).lower().replace('(', '').replace(')', ''))\n",
        "df.show(5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbYb22t_ZIRq"
      },
      "source": [
        "Having tried to find a solution, did you get something like this? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZi-fQLRZIRq",
        "outputId": "7170933b-8b06-42e4-ba18-f0d37b85cfba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+----+----+----+-----+----------+---------------+--------------+----+\n",
            "|          timestamp|open|high| low|close|volume_btc|volume_currency|weighted_price|year|\n",
            "+-------------------+----+----+----+-----+----------+---------------+--------------+----+\n",
            "|2011-12-31 07:52:00|4.39|4.39|4.39| 4.39|0.45558086|            2.0|          4.39|2011|\n",
            "|2011-12-31 07:53:00| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011|\n",
            "|2011-12-31 07:54:00| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011|\n",
            "|2011-12-31 07:55:00| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011|\n",
            "|2011-12-31 07:56:00| NaN| NaN| NaN|  NaN|       NaN|            NaN|           NaN|2011|\n",
            "+-------------------+----+----+----+-----+----------+---------------+--------------+----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for column in df.columns:\n",
        "    df = df.withColumnRenamed(column, '_'.join(column.split()).lower().replace('(', '').replace(')', ''))\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbqOZsKRZIRq"
      },
      "source": [
        "### Dropping  a column\n",
        "\n",
        "Similar to column renaming, it is often required or desirable for us to remove a column(s) from our DataFrame. Some of these reasons may include: \n",
        "\n",
        " - **Removing duplicate data:** Some processes within our data pipeline may cause columns to become duplicated. For example, when performing a join between DataFrames in Spark, an artefact of the operation is the duplication of the joining column which appears twice within the result. Dropping one of these columns removes the needless redundancy within our data.\n",
        " - **Pruning our data:** Another by-product of processes within our data pipeline may be the creation of features that aren't duplicated, but are irrelevant or unusable for our needs. Examples of this could include the dropping of a non-key field generated by default from an [IOT](https://en.wikipedia.org/wiki/Internet_of_things) device or removing a column that contains too high a proportion of missing data.    \n",
        "\n",
        "To remove one or more columns from a DataFrame, simply call its `.drop()` method, which accepts a comma-separated list of strings indicating the names of the columns to be removed. \n",
        "\n",
        "For example: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJ7oZ8gFZIRr",
        "outputId": "e7a2d1b6-3148-4312-f86f-7d1ef2523edd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+----------+---------------+--------------+----+-----+\n",
            "|          timestamp|volume_btc|volume_currency|weighted_price|year|month|\n",
            "+-------------------+----------+---------------+--------------+----+-----+\n",
            "|2011-12-31 07:52:00|0.45558086|            2.0|          4.39|2011|   12|\n",
            "|2011-12-31 07:53:00|       NaN|            NaN|           NaN|2011|   12|\n",
            "|2011-12-31 07:54:00|       NaN|            NaN|           NaN|2011|   12|\n",
            "|2011-12-31 07:55:00|       NaN|            NaN|           NaN|2011|   12|\n",
            "|2011-12-31 07:56:00|       NaN|            NaN|           NaN|2011|   12|\n",
            "+-------------------+----------+---------------+--------------+----+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.drop('Open', 'Close', 'High', 'Low').show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66ASNBzoZIRr"
      },
      "source": [
        "Again, it's important to remember that, due to the immutability of DataFrames, the result of the `drop` operation needs to be assigned to a new or existing variable to be preserved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7L1fO2I3ZIRr"
      },
      "source": [
        "## Aggregations\n",
        "\n",
        "We've learned many important skills up until this point. We can now load data into Spark from an external source, change its structure, create additional fields, and view various projections representing filtered versions of its content. Now, we're going to learn one last basic skill which will prove vital in our ability to analyse data in Spark – performing data **aggregations**. \n",
        "\n",
        "The concept of performing an aggregation shouldn't be unfamiliar to us; we've come across these operations in both SQL and Pandas (Python-based DataFrames). Thankfully, we'll soon see that performing an aggregation using PySpark is very similar to these other frameworks.\n",
        "\n",
        "Remember that when we perform an aggregation, we collect or *'group'* data based on a specified field, and then calculate an aggregate result using a given function. This means that our aggregation operations are inherently composed of two parts: \n",
        "\n",
        " - The **Group stage**, where we designate which field(s) we should aggregate over; and\n",
        " - The **Function application stage**, where we specify an aggregation function to apply to our grouped data. \n",
        " \n",
        "In PySpark, both of these stages can be applied using distinct methods. \n",
        "\n",
        "Let's first consider the *grouping stage* of an aggregation. To group data together, we call the `groupBy()` method of a DataFrame. This method accepts one or more columns within our DataFrame and returns a [`RelationalGroupedDataset`](https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-RelationalGroupedDataset.html) object which can have aggregations applied to it. \n",
        "\n",
        "To see this in action, let's group our data according to the year it was captured. We could do this by executing the following statement:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2newCGSsZIRs",
        "outputId": "d69fda5a-8404-41d5-d192-32f3b51be001"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.group.GroupedData at 0x7f406b54d810>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "df.groupBy(F.year('timestamp'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxSNcCT1ZIRs"
      },
      "source": [
        "Interesting. As we mentioned above, the result of the `groupBy()` method isn't a DataFrame as we've previously seen, but is instead a `RelationalGroupedDataset` object with specific properties. In fact, we can't actually visualise this object to get a result from it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "noZooKuYZIRs",
        "outputId": "c3bc2eec-3ffe-4def-e6a4-0294a6a87a9c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-20e299b16339>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Calling .show() on the GroupedData object will result in an error!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'GroupedData' object has no attribute 'show'"
          ]
        }
      ],
      "source": [
        "# Calling .show() on the GroupedData object will result in an error! \n",
        "df.groupBy(F.year('timestamp')).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD2QhRpGZIRt"
      },
      "source": [
        "This is where the *aggregation function application* stage comes in. It allows us to transform our grouped data object into a calculated result, producing a DataFrame once more. \n",
        "\n",
        "There are several aggregation functions that we can choose from. These are all specified within and available from our trusty [`functions` module](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions), and include: \n",
        " - `min()`: Return the smallest value found in the grouped data;\n",
        " - `max()`: Return the largest value found in the grouped data;\n",
        " - `sum()`: Return the sum of all values in the grouped data; and\n",
        " - `mean()`: Return the average of all values in the grouped data. \n",
        " \n",
        "Using this knowledge, let's complete our example from above by counting the number of records that have been recorded for each year within the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "678hUyFrZIRt",
        "outputId": "d43e149d-3876-4336-8f68-a4c748cb0638"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+------+\n",
            "|year(timestamp)| count|\n",
            "+---------------+------+\n",
            "|           2018|525600|\n",
            "|           2015|519128|\n",
            "|           2013|525600|\n",
            "|           2014|525600|\n",
            "|           2019|525600|\n",
            "|           2020|527040|\n",
            "|           2012|527040|\n",
            "|           2016|527040|\n",
            "|           2011|   968|\n",
            "|           2017|525600|\n",
            "|           2021|128161|\n",
            "+---------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We can call the .count method on the result of our .groupBy operation.\n",
        "df.groupBy(F.year('timestamp')).count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYEq-LIzZIRt"
      },
      "source": [
        "Looks good! \n",
        "\n",
        "It's important to note that we don't always have to use the `groupBy()` function to calculate an aggregate of a column's data. In this sense, there are many aggregation methods which can be called directly from a DataFrame, including `stat()`, `correlation()`, `covariance()`, `sampleBy()`, `approxQuantile()` and `frequentItems()`.\n",
        "\n",
        "We also aren't restricted to call a single aggregation function on our grouped data. Instead, we can make use of the powerful `agg()` method, which allows us to create multiple aggregations by supplying a dictionary object of all transformations we wish to perform on our DataFrame. Here the basic structure of the dictionary should be `{field: aggregation_function}`, for example, `{'high': 'sum'}`. \n",
        "\n",
        "Let's see a practical example of this: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apPkJp4nZIRu",
        "outputId": "922eae1e-9a55-4efe-c52d-45bab8637ee1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+---------+---------------------+\n",
            "|year(timestamp)|max(high)|sum(volume_currency) |\n",
            "+---------------+---------+---------------------+\n",
            "|2018           |17234.99 |3.0746672203400585E10|\n",
            "|2015           |502.0    |1.545444530137112E9  |\n",
            "|2013           |1163.0   |1.5511774873354862E9 |\n",
            "|2014           |995.0    |2.6152928091775055E9 |\n",
            "|2019           |13880.0  |2.3158536096590027E10|\n",
            "|2020           |29300.0  |3.3290575546295395E10|\n",
            "|2012           |16.41    |5764800.229803922    |\n",
            "|2016           |980.74   |1.1124637970242171E9 |\n",
            "|2011           |4.58     |425.3203430175781    |\n",
            "|2017           |19666.0  |2.1775822445412468E10|\n",
            "+---------------+---------+---------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Note that we use the .dropna() method to remove all NaN values\n",
        "# from the dataset which would otherwise cause all our results \n",
        "# to become NaN. \n",
        "df.dropna().groupBy(F.year('timestamp')) \\\n",
        "  .agg({'high': 'max', 'volume_currency': 'sum'}) \\\n",
        "  .show(10, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2OWsWbRZIRu"
      },
      "source": [
        "Looking at these results, we can make two observations: \n",
        "\n",
        "Firstly, *the names of the aggregated columns conform to a specific format*, namely `aggregation_function(column_name)`. For example, using the `max` function on the `'high'` column produces a result column called `'max(high)'`. \n",
        "\n",
        "Secondly, it becomes apparent that any trends in our results are difficult to spot due to a lack of order. This is where the `orderBy()` method saves the day. Almost identical to its SQL counterpart, in Spark the `orderBy()` method accepts a column name used to order the aggregation output either in ascending (default behaviour) or descending order.\n",
        "\n",
        "We can extend our example once again by adding in ordering:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kb6dbirpZIRv",
        "outputId": "8a5dd57d-bd5f-49f1-be4d-83971d5ecef2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+---------+---------------------+\n",
            "|year(timestamp)|max(high)|sum(volume_currency) |\n",
            "+---------------+---------+---------------------+\n",
            "|2011           |4.58     |425.3203430175781    |\n",
            "|2012           |16.41    |5764800.229803922    |\n",
            "|2013           |1163.0   |1.5511774873354862E9 |\n",
            "|2014           |995.0    |2.6152928091775055E9 |\n",
            "|2015           |502.0    |1.545444530137112E9  |\n",
            "|2016           |980.74   |1.1124637970242171E9 |\n",
            "|2017           |19666.0  |2.1775822445412468E10|\n",
            "|2018           |17234.99 |3.0746672203400585E10|\n",
            "|2019           |13880.0  |2.3158536096590027E10|\n",
            "|2020           |29300.0  |3.3290575546295395E10|\n",
            "+---------------+---------+---------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Take note that, because of Spark's naming convention, \n",
        "# we order by 'year(timestamp)' instead of just 'timestamp'.\n",
        "df.dropna().groupBy(F.year('timestamp')) \\\n",
        "    .agg({'high': 'max', 'volume_currency': 'sum'}) \\\n",
        "    .orderBy('year(timestamp)').show(10, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exZtwVZGZIRv"
      },
      "source": [
        "As can be seen in the code above, Spark's naming convention can become pretty unwieldy quickly. Here if we were to string multiple aggregations together on a single field, it could result in something like `agg_3(agg_2(agg_1(field_name)))`. Only a mother could love that.\n",
        "\n",
        "Spark helps us prevent such a tragedy by providing the `alias()` method, which allows us to specify what the result of an aggregation should be called. To create aliases, we use a select statement to get the columns that we want to rename, followed by using the `col()` method to retrieve the desired columns, and `alias()` method to rename them. As we've seen by now, Spark gives us multiple ways to accomplish something. You can also use the `withColumnRenamed()` method here instead of `alias()`. In most cases, it will depend on the use case while optimising succinctness, which determines what our approach should be. \n",
        "\n",
        "Let's put this all together by creating one final aggregation to give us a sense of Bitcoin's behaviour over the last 10 years:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1mbts9zZIRv",
        "outputId": "5a5a6d27-a194-4990-c4b2-cbc0532b7749"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------+---------------------+\n",
            "|year_agg|max     |volume_sum           |\n",
            "+--------+--------+---------------------+\n",
            "|2011    |4.58    |425.3203430175781    |\n",
            "|2012    |16.41   |5764800.229803922    |\n",
            "|2013    |1163.0  |1.5511774873354862E9 |\n",
            "|2014    |995.0   |2.6152928091775055E9 |\n",
            "|2015    |502.0   |1.545444530137112E9  |\n",
            "|2016    |980.74  |1.1124637970242171E9 |\n",
            "|2017    |19666.0 |2.1775822445412468E10|\n",
            "|2018    |17234.99|3.0746672203400585E10|\n",
            "|2019    |13880.0 |2.3158536096590027E10|\n",
            "|2020    |29300.0 |3.3290575546295395E10|\n",
            "+--------+--------+---------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We chain the .alias() method within a select statement in order \n",
        "# to rename our aggregated results. \n",
        "df.dropna().groupBy(F.year('timestamp'))\\\n",
        "    .agg({'high': 'max', 'volume_currency': 'sum'})\\\n",
        "    .select(F.col('year(timestamp)').alias('year_agg'), \n",
        "            F.col('max(high)').alias('max'), \n",
        "            F.col('sum(volume_currency)').alias('volume_sum'))\\\n",
        "    .orderBy('year(timestamp)').show(10, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOS0x91LZIRw"
      },
      "source": [
        "This is quite informative. At first, we can observe Bitcoin's peak in 2017 where the digital currency maxed out just under 20 000 USD, then its subsequent crash, and finally, its recent climb and resurgence to a new high of just under 30 000 USD in 2020. We can also see a correlation between Bitcoin's trading value and the volume of trades over time. This isn't bad, considering we've used just a few basic Spark commands to analyse our data.  \n",
        "\n",
        "With our analysis complete, it's time to learn how to store our data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKonKERCZIRw"
      },
      "source": [
        "## Writing data from the environment using PySpark\n",
        "\n",
        "Typically, once we've performed some transformation on our dataset, we will want to persist our results by writing/saving them to an external data source. \n",
        "\n",
        "Spark supports a wide range of data formats, including CSV, JSON, Avro, and Parquet. For more information on the formats supported, see the [Spark sources documentation](https://spark.apache.org/docs/latest/sql-data-sources.html). A common choice as a storage format is Parquet, as it enables us to save our table's schema as metadata, allowing us to skip the tedious process of defining the data schema whenever our data is re-read into Spark.     \n",
        "\n",
        "To save our data to a specific source, we use the `.write()` method of a DataFrame, which invokes the [DataFrameWriter](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=save#pyspark.sql.DataFrameWriter) class, allowing us to configure the write process. Just like the read operation that we saw at the start of this train, when we perform a data write we can call the `.format()` and `.option()` methods to alter the behaviour of the write operation before it is executed. Two unique write methods to remember are: \n",
        "\n",
        " - **`save()`** which specifies a write location in storage (for example, `df.write.format(\"parquet\").save(parquet_path)`); and\n",
        " - **`saveAsTable()`** which saves our data to a table, along with its metadata being written to the Hive metastore (for example, `df.write.format(\"parquet\").saveAsTable(parquet_table)`). \n",
        "\n",
        "Related to the second method, remember that the Hive metastore is an abstraction built on top of the Hadoop file system (HDFS), but which also comes standard with a Spark installation. This allows us to store additional metadata along with our files, such as any predefined properties. Any table created using the above syntax will be created in `/user/hive/warehouse/{schema_name}.db/{table_name}`.\n",
        "\n",
        "This table can also be created using Spark SQL (which is covered in a related train):\n",
        "\n",
        "```SQL\n",
        "CREATE TABLE {schema_name}.{table_name}\n",
        "    ({table_schema})\n",
        "    LOCATION \"/user/hive/warehouse/{schema_name}.db/{table_name}\"\n",
        "```\n",
        "\n",
        "Additional properties can be added to the table using:\n",
        "\n",
        "```SQL\n",
        "ALTER TABLE {schema_name}.{table_name} \n",
        "    SET TBLPROPERTIES (PROPERTIES = \"{properties}\")\n",
        "```\n",
        "    \n",
        "\n",
        "That's enough theory for now. Let's cement our learning by writing our data to the same directory from which it was read, using the Parquet file format.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "e3xYNC3-ZIRx"
      },
      "outputs": [],
      "source": [
        "# Just as we did in the read step of our data, ensure that the save path \n",
        "# given below exists and is accessible from this notebook. \n",
        "df.write.format(\"parquet\").save(\"./data/bitcoin_data/transformed_data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsXzQTO1ZIRx"
      },
      "source": [
        "If the command didn't return any errors, then you've just successfully written data as a Parquet file – well done! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRwt5Mh7ZIRy"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "That was a lot to take in. Let's do a brief recap to remind us what we've learned. \n",
        "\n",
        "We started with an introduction to the PySpark API, which provides an interface to Spark via the Python programming language. We were then able to use PySpark's functionality to read in data for visualisation and processing, during which time we were also exposed to syntax features such as command chaining which can help make our code more succinct. \n",
        "\n",
        "Once read into Spark, we were able to manipulate our data in multiple ways; creating new columns, renaming existing ones, forming new projections through the application of filters, as well as analysing our data by performing aggregations on it. \n",
        "\n",
        "We then wrapped up by briefly discussing the various options available for writing our data from Spark to another source. \n",
        "\n",
        "We encourage you to practise the skills which we've covered within this train by trying to analyse your own sources of data that you may find. Doing so will ensure that you don't have to worry about small distractions such as syntax when you face more challenging scenarios as a data engineer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kX18FfEmZIRy"
      },
      "source": [
        "## Appendix\n",
        "\n",
        "The following resources may prove helpful on your learning journey with PySpark:\n",
        "\n",
        " - [Six Comprehensive Spark Exercises to Practice](https://towardsdatascience.com/six-spark-exercises-to-rule-them-all-242445b24565)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEIUp4ZrZIRy"
      },
      "source": [
        "### Exercise solutions \n",
        "\n",
        "**\\[Exercise 1\\]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rt2h6RkWZIR3"
      },
      "outputs": [],
      "source": [
        "# Part 1\n",
        "# Write your code to add a 'Month' column to the DataFrame here: \n",
        "\n",
        "df_ex1 = df.withColumn('Month', F.month(F.col('Timestamp')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmTfEiFiZIR3"
      },
      "outputs": [],
      "source": [
        "# Part 2\n",
        "# Write your code to add a 'Weekday' column to the DataFrame here: \n",
        "# Note that the weekday can be represented as an integer, i.e. Sunday --> 1, Saturday --> 7.\n",
        "\n",
        "df_ex1 = df_ex1.withColumn('Weekday', F.dayofweek(F.col('Timestamp')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8AnqwlIZIR4"
      },
      "source": [
        "**\\[Exercise 2\\]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GMrGzYrZIR4"
      },
      "outputs": [],
      "source": [
        "# Write your code to define your data projection here:\n",
        "\n",
        "new_filter_date = date(2018, 1, 1)\n",
        "filter_1 = F.dayofweek(F.col('Timestamp')) == 2\n",
        "filter_3 = F.col('Timestamp') > new_filter_date\n",
        "filter_2 = F.col('Close') < 6000\n",
        "df_ex2 = df.select('Timestamp', 'Close').where((filter_1) & (filter_2) & (filter_3))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Common_spark_transformations.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}